{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CwEx2Update.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikoszav/MathForAI/blob/main/Ex2/CwEx2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKjL2MtBY5lz"
      },
      "source": [
        "# ANN with Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr1xS30tbIuk"
      },
      "source": [
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random as rd\r\n",
        "import _pickle as cPickle    \r\n",
        "import gzip\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWuL-N8jZJ1M"
      },
      "source": [
        "#Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE6NsYKUUFsk"
      },
      "source": [
        "#load the file with the data \n",
        "#put the path of the file mnist.pkl.gz below\n",
        "mnist_file = gzip.open('/content/drive/MyDrive/dataset/mnist.pkl.gz', 'rb') \n",
        "#load them and split them in training_data, validation_data, test_data\n",
        "trainingData, validationData, testData = cPickle.load(mnist_file, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v05OkbaZHCr"
      },
      "source": [
        "# One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XuPZkUvesCi"
      },
      "source": [
        "#return a vector of 10 on and in the position of the number(class) is make it equal with 1(one hot encoding)\n",
        "def yLabel(y):\n",
        "    #initialize an array of length 10 with zero\n",
        "    yLabel = np.zeros((10,1))\n",
        "    \n",
        "    #at y position it make it equals with 1\n",
        "    yLabel[y] = 1.0\n",
        "    \n",
        "    return yLabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEI9M9UoZRiP"
      },
      "source": [
        "#Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FGbA5qDNhy0"
      },
      "source": [
        "#basic preprocsing to make our data in a way that our algorithm can undestand the data\r\n",
        "trainingInputs = [np.reshape(x, (784,1)) for x in trainingData[0]]\r\n",
        "trainingLabels = [yLabel(y) for y in trainingData[1]]\r\n",
        "\r\n",
        "trainSet = zip(trainingInputs, trainingLabels)\r\n",
        "    \r\n",
        "validationInputs = [np.reshape(x, (784,1)) for x in validationData[0]]\r\n",
        "validationSet = zip(validationInputs, validationData[1])\r\n",
        "\r\n",
        "testInputs = [np.reshape(x, (784,1)) for x in testData[0]]\r\n",
        "testSet = zip(testInputs, testData[1])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw6IdOPUfAMe"
      },
      "source": [
        "trainSet = list(trainSet)\n",
        "validationSet = list(validationSet)\n",
        "testSet = list(testSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0KFhg0LZVds"
      },
      "source": [
        "#Import functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRiJTQw8aN3X"
      },
      "source": [
        "!python3 functions.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1llmBsHaN3b",
        "outputId": "5ffa355e-490f-4a5c-93ab-33a728944a86"
      },
      "source": [
        "!pip install ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipynb in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Be0PXl1bVjN"
      },
      "source": [
        "import ipynb.fs \r\n",
        "from .defs.functions import relu\r\n",
        "from .defs.functions import reluDerivative\r\n",
        "from .defs.functions import softmax\r\n",
        "from .defs.functions import sigmoid\r\n",
        "from .defs.functions import derivativeSigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFFa6Vh-QlD_"
      },
      "source": [
        "#best softmax at the backprop \r\n",
        "\r\n",
        "#unitilization gausian distibution\r\n",
        "'''\r\n",
        "The classifier class:\r\n",
        "in the constuctor we have the size of the nerwork, a list with the activation functions for each one of the hidden layers, the untis of the inputlayer(784),\r\n",
        "a list with the hidden layers and the the numbers of units for each hidden layer, the units of the ouput layer(10)  \r\n",
        "'''\r\n",
        "class Classifier():\r\n",
        "  #define the constractor for the classifier \r\n",
        "  #define layers, biases and weights\r\n",
        "  def __init__(self, sizeNetwork, activationFunction, inputLayer, hiddenlayerList, outputLayer):\r\n",
        "    #create a list which will contain the differents sizes og the layers of the network\r\n",
        "    self.neuralNetworkParameters = [inputLayer]\r\n",
        "    for hiddenLayer in hiddenlayerList:\r\n",
        "      self.neuralNetworkParameters.append(hiddenLayer)\r\n",
        "\r\n",
        "    self.neuralNetworkParameters.append(outputLayer)\r\n",
        "    #define the number of layers the the neural network will have\r\n",
        "    self.layers = sizeNetwork\r\n",
        "\r\n",
        "    #initialize the biases of the neural network\r\n",
        "    self.biases = [np.random.randn(y,1) for y in self.neuralNetworkParameters[1:]]\r\n",
        "    #initilize the weights of the neural network\r\n",
        "    self.weights = [np.random.randn(y,x) / np.sqrt(x) for x, y in zip(self.neuralNetworkParameters[:-1], self.neuralNetworkParameters[1:])]\r\n",
        "    #define the activation function\r\n",
        "    self.f = activationFunction\r\n",
        "    #initliaze the cost function\r\n",
        "    self.training_accuracy, self.training_cost, self.validationAcc = [], [], []\r\n",
        " \r\n",
        "  #define the feedforward function \r\n",
        "  def feedforward(self, currentNeuron):\r\n",
        "    i = 0\r\n",
        "    for b, w in zip(self.biases, self.weights):     \r\n",
        "      if i != len(hiddenLayerList):\r\n",
        "        # print(f[i])\r\n",
        "\r\n",
        "        linearFun = np.dot(w, currentNeuron)+b\r\n",
        "        \r\n",
        "        currentNeuron = self.activationFun(linearFun,self.f[i])\r\n",
        "\r\n",
        "      \r\n",
        "      else:\r\n",
        "        currentNeuron = np.dot(w, currentNeuron)+b\r\n",
        "        currentNeuron = np.exp(currentNeuron) / np.sum(np.exp(currentNeuron), axis=0)\r\n",
        "\r\n",
        "    i+=1\r\n",
        "    return currentNeuron\r\n",
        "\r\n",
        "  # implematation of minibatch stochastic gradient descent\r\n",
        "  def fit(self, trainSet, testSet, validationSet, learningRate, epochs, miniBatchSize):    \r\n",
        "    \r\n",
        "    trainSet = list(trainSet)\r\n",
        "    n_train = len(trainSet)\r\n",
        "    #speify the length of the trainset \r\n",
        "    lengthOfTrainData = len(trainSet)\r\n",
        "    # lengthOfTestData = len(testSet)\r\n",
        "    epoch =1\r\n",
        "    continue_training = True\r\n",
        "    #define and implement mini batch gradient descent \r\n",
        "   \r\n",
        "    while continue_training:\r\n",
        "      #shuffle data \r\n",
        "      random.shuffle(trainSet)\r\n",
        "      \r\n",
        "      miniBatches = [trainSet[y: y+miniBatchSize] for y in range(0, lengthOfTrainData, miniBatchSize)]\r\n",
        "      for miniBatch in miniBatches:\r\n",
        "        # need to update weights and biases in respect of the learning rate         \r\n",
        "        self.updateParameters(miniBatch, learningRate, n_train)\r\n",
        "      accuracy = self.accuracy(trainSet)\r\n",
        "      print(\"Epoch: \", epoch , \": \", accuracy , \" / \", lengthOfTrainData )\r\n",
        "      self.training_accuracy.append(accuracy)\r\n",
        "      self.training_cost.append(self.costentropy(trainSet))\r\n",
        "\r\n",
        "      accuracy = (accuracy/len(trainSet)) * 100\r\n",
        "      val_acc = self.evaluate(validationSet, valSet= True)\r\n",
        "      self.validationAcc.append(val_acc)\r\n",
        "      if val_acc > 95.0:\r\n",
        "        continue_training = False\r\n",
        "        print(\"Early finish after \", epoch, \" epochs with training accuracy be \", accuracy)\r\n",
        "      elif epoch >= epochs: \r\n",
        "        continue_training = False\r\n",
        "        print(\"Finish after \", epoch, \" epochs with training accuracy be \", accuracy)\r\n",
        "      epoch += 1\r\n",
        "  \r\n",
        "  #function for updating parameters \r\n",
        "  def updateParameters(self,miniBatch, learningRate, n_train):\r\n",
        "    #initliaze the lists which will store the weights and biases\r\n",
        "    lmbda = 5.0\r\n",
        "    listOfWeights = [np.zeros(weight.shape) for weight in self.weights] \r\n",
        "    listOfBiases = [np.zeros(bias.shape) for bias in self.biases] \r\n",
        "\r\n",
        "    for X, y in miniBatch:\r\n",
        "      #the change of the weights and the biases after the backpropagation process \r\n",
        "      deltaListOfWeights, deltaListOfBiases = self.backpropagation(X, y)\r\n",
        "      \r\n",
        "      #update the list of biases\r\n",
        "      listOfBiases = [b + dB for b, dB in zip(listOfBiases , deltaListOfBiases)]\r\n",
        "      #update the list of weights\r\n",
        "      listOfWeights = [w + dW for w, dW in zip(listOfWeights , deltaListOfWeights)]\r\n",
        "\r\n",
        "    #updating the weights by using gradient descent\r\n",
        "    # self.weights = [(1-learningRate*(lmbda/n_train)) * w - (learningRate/len(miniBatch)) * nw for w, nw in zip(self.weights, listOfWeights)]\r\n",
        "    self.weights = [w - (learningRate/len(miniBatch)) * nw for w, nw in zip(self.weights, listOfWeights)]\r\n",
        "    #updating the biases by using gradient descent\r\n",
        "    self.biases = [b - (learningRate/len(miniBatch)) * nb for b, nb in zip(self.biases, listOfBiases)]\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  def backpropagation(self, X, y):\r\n",
        "    deltaW = [np.zeros(w.shape) for w in self.weights]\r\n",
        "    deltaB = [np.zeros(b.shape) for b in self.biases]\r\n",
        "    \r\n",
        "\r\n",
        "    # implematation of feedfard process calculate linear model + activation function\r\n",
        "    resultActivation = X\r\n",
        "\r\n",
        "    #will keep all the results of the neurons layer by layer\r\n",
        "    resultsActivation = [X]   \r\n",
        "    \r\n",
        "    #keep a list of z vector the output of a neuron \r\n",
        "    zs = []\r\n",
        "    i = 0\r\n",
        "    for b, w in zip(self.biases, self.weights):\r\n",
        "    \r\n",
        "      if i < len(hiddenLayerList):\r\n",
        "        #calculating z vectors by using the theory\r\n",
        "        #from theory we know that the output of a node is equal with the dot product of the weights and the previous nodes + the bias term      \r\n",
        "        z = np.matmul(w, resultActivation) + b        \r\n",
        "        #add z to the list \r\n",
        "        zs.append(z)\r\n",
        "        # print(self.f[i])\r\n",
        "        #apply the agtivation funnction that we have defined \r\n",
        "        resultActivation = self.activationFun(z, self.f[i])\r\n",
        "        #store this result in our list\r\n",
        "        resultsActivation.append(resultActivation)\r\n",
        "      \r\n",
        "      else:\r\n",
        "        z = np.matmul(w, resultActivation) + b       \r\n",
        "        #add z to the list \r\n",
        "        zs.append(z)\r\n",
        "        #softmax for the last layer\r\n",
        "        z = np.exp(z) / np.sum(np.exp(z), axis=0)\r\n",
        "        resultsActivation.append(z)\r\n",
        "      i += 1\r\n",
        "      # Implematation of the backward procces \r\n",
        "\r\n",
        "    error = resultsActivation[-1] - y \r\n",
        "\r\n",
        "    # print(len(zs[-1]))\r\n",
        "    deltaError = error * self.derivativeActivationFun(zs[-1], self.f[-1])\r\n",
        "    # print(self.derivativeActivationFun(zs[-1]))\r\n",
        "    deltaB[-1] = np.mean(deltaError)\r\n",
        "    deltaW[-1] = np.dot(deltaError, resultsActivation[-2].transpose())\r\n",
        "    \r\n",
        "    for layer in range(2,self.layers):\r\n",
        "      z = zs[-layer]\r\n",
        "      deltaPred = self.derivativeActivationFun(z, self.f[layer-2])\r\n",
        "      deltaError = np.matmul(self.weights[-layer + 1].transpose(), deltaError) * deltaPred\r\n",
        "      deltaB[-layer] = np.mean(deltaError)\r\n",
        "      deltaW[-layer] = np.dot(deltaError, resultsActivation[-layer - 1].transpose()) \r\n",
        "\r\n",
        "      \r\n",
        "    return (deltaW, deltaB)\r\n",
        "\r\n",
        "\r\n",
        "#Functions \r\n",
        "\r\n",
        "  #define the actuvation function that you choose\r\n",
        "  def activationFun(self, x, f):\r\n",
        "    if (f == \"sigmoid\"):\r\n",
        "      return sigmoid(x)\r\n",
        "    elif (f == \"relu\"):     \r\n",
        "      # print(\"okay\") \r\n",
        "      return relu(x)  \r\n",
        "  \r\n",
        "  #define the derivative of the actuvation function that you choose\r\n",
        "  def derivativeActivationFun(self, x, f):\r\n",
        "    if (f == \"sigmoid\"):\r\n",
        "      return derivativeSigmoid(x)\r\n",
        "    elif (f == \"relu\"):\r\n",
        "      return reluDerivative(x)  \r\n",
        "  \r\n",
        "\r\n",
        "  #calculate the accuracy of the classifier on the test set\r\n",
        "  def accuracy(self, data):\r\n",
        "    accuracy = 0\r\n",
        "    #create a list od tuples where first element is the predicted y and second element the actually output\r\n",
        "    results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x,y) in data]\r\n",
        "    acc = sum(int(x == y) for (x,y) in results)\r\n",
        "    accuracy = acc/len(data) * 100\r\n",
        "    print('Accuracy on Training data set: ', accuracy)\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "\r\n",
        "  #method to evaluate the classifier \r\n",
        "  def evaluate(self, data, valSet):\r\n",
        "    if valSet:\r\n",
        "      #create a list od tuples where first element is the predicted y and second element the actually output\r\n",
        "      results = [(np.argmax(self.feedforward(x)), y) for (x,y) in data]\r\n",
        "      accuracy = sum(int(x == y) for (x,y) in results)\r\n",
        "      accuracy = accuracy/len(data) * 100\r\n",
        "      print('Accuracy on validatation data set: ', accuracy)\r\n",
        "      return accuracy\r\n",
        "    else:\r\n",
        "      #create a list od tuples where first element is the predicted y and second element the actually output\r\n",
        "      results = [(np.argmax(self.feedforward(x)), y) for (x,y) in data]\r\n",
        "      accuracy = sum(int(x == y) for (x,y) in results)\r\n",
        "      \r\n",
        "      accuracy = accuracy/len(data) * 100\r\n",
        "      print('Accuracy on test data set: ', accuracy)\r\n",
        "\r\n",
        "\r\n",
        "  #calculate the cost based on the costentropy function \r\n",
        "  def costentropy(self, data):\r\n",
        "    cost = 0.0\r\n",
        "    for x, y in data:\r\n",
        "        y_pred = self.feedforward(x)\r\n",
        "        y_pred = softmax(y_pred)\r\n",
        "        \r\n",
        "        # print(y_pred, y)\r\n",
        "        # y_pred\r\n",
        "        cost = np.mean(-y * np.log(y_pred)- (1.0-y) * np.log(1.0 - y_pred))\r\n",
        "    \r\n",
        "    print(cost)\r\n",
        "    return cost\r\n",
        "\r\n",
        "  def matrices(self):\r\n",
        "    return self.training_accuracy, self.training_cost, self.validationAcc   \r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN3BlSaVbS0g"
      },
      "source": [
        "#specify the number of units for each of the hidden layer\r\n",
        "#if you want more than one layers you can fo smt like this hiddenLayerList = [1000, 100, 10] 3 hidden layer and the first has 1000units, second 100, third 10\r\n",
        "hiddenLayerList = [1000]\r\n",
        "#the activation function the size of this list must map the hidden layer list \r\n",
        "activationFun = [\"sigmoid\"]\r\n",
        "#call the class\r\n",
        "net = Classifier(3, activationFun, 784, hiddenLayerList,10)\r\n",
        "#call the fit method \r\n",
        "net.fit(trainSet, testSet, validationSet, learningRate= 0.01, epochs = 30, miniBatchSize= 10)\r\n",
        "#return the accuracies lists\r\n",
        "trainingAcc, trainingLoss, validationAcc  = net.matrices()\r\n",
        "#evaluate on the test set\r\n",
        "net.evaluate(testSet, valSet= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIFdYHZkbacn"
      },
      "source": [
        "#create dictionary which store the accuracies and the costs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnUHQOWEkj_S"
      },
      "source": [
        "evaluation = {}\r\n",
        "evaluation[\"trainAccSigmoid\"] = trainingAcc\r\n",
        "evaluation[\"trainAccRelu\"] = trainingAcc1\r\n",
        "\r\n",
        "evaluation[\"trainLossSigmoid\"] = trainingLoss\r\n",
        "evaluation[\"trainLossRelu\"] = trainingLoss1\r\n",
        "\r\n",
        "evaluation[\"validationAccSigmoid\"] = validationAcc\r\n",
        "evaluation[\"validationAccRelu\"] = validationAcc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ678Ugdbs_r"
      },
      "source": [
        "#Plot graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-I4Wk5v2mD6"
      },
      "source": [
        "plt.plot(evaluation[\"trainAccSigmoid\"], label='sigmoid')\r\n",
        "plt.plot(evaluation[\"trainAccRelu\"], label='relu')\r\n",
        "plt.ylabel('Training Accuracy')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.xticks(np.arange(0,32,2))\r\n",
        "plt.title('Training Accuracy per epoch')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ETUWpO920Bi"
      },
      "source": [
        "plt.plot(evaluation[\"trainLossSigmoid\"], label='sigmoid')\r\n",
        "plt.plot(evaluation[\"trainLossRelu\"], label='relu')\r\n",
        "plt.ylabel('Training loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.xticks(np.arange(0,32,2))\r\n",
        "plt.title('Training loss per epoch')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PydDsIhK3BG4"
      },
      "source": [
        "plt.plot(evaluation[\"validationAccSigmoid\"], label = 'sigmoid')\r\n",
        "plt.plot(evaluation[\"validationAccRelu\"], label = 'relu') \r\n",
        "plt.ylabel('validation accuracy')\r\n",
        "plt.xticks(np.arange(0,32,2))\r\n",
        "plt.xlabel('Epochs')   \r\n",
        "plt.title('Validation accuracy per epoch')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}